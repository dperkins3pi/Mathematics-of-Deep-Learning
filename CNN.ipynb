{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMRswcCEPI3vv/fjywvBIkF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"He0BJAcU8ty_"},"outputs":[],"source":["from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, ConcatDataset, random_split\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F"]},{"cell_type":"code","source":["# Load in the data\n","mnist_train = datasets.MNIST(\"./data\", train=True, download=True, transform=transforms.ToTensor())\n","mnist_test = datasets.MNIST(\"./data\", train=False, download=True, transform=transforms.ToTensor())\n","combined_dataset = ConcatDataset([mnist_train, mnist_test])   # Combine the training and test datasets\n","\n","# Define the size of the new training and test sets (80% training, 20% testing)\n","train_size = int(0.8 * len(combined_dataset))  # 80% of the data\n","test_size = len(combined_dataset) - train_size  # The remaining 20% for testing\n","train_dataset, test_dataset = random_split(combined_dataset, [train_size, test_size])\n","\n","# Only use 0's and 1's\n","train_idx = mnist_train.targets <= 1 #only retrieve those with labels less than this value\n","mnist_train.data = mnist_train.data[train_idx]\n","mnist_train.targets = mnist_train.targets[train_idx]\n","\n","test_idx = mnist_test.targets <= 1 #only retrieve those with labels less than this value\n","mnist_test.data = mnist_test.data[test_idx]\n","mnist_test.targets = mnist_test.targets[test_idx]\n","\n","# Make data loaders\n","train_loader = DataLoader(mnist_train, batch_size = 100, shuffle=True)\n","test_loader = DataLoader(mnist_test, batch_size = 100, shuffle=False)"],"metadata":{"id":"M1NahBzO9aHN","executionInfo":{"status":"ok","timestamp":1726529010845,"user_tz":360,"elapsed":6555,"user":{"displayName":"Danny Perkins","userId":"13093065982105422940"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"60f3a912-c0b8-4b4e-adf5-fa814295e701"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 16191419.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 488517.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 4435569.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 3826175.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n"]}]},{"cell_type":"code","source":["# do a single pass over the data\n","def epoch(loader, model, opt=None):\n","    total_loss, total_err = 0., 0.\n","    for X, y in loader:\n","        y_hat = model(X.view(X.shape[0], -1))[:,0]  # Make prediction\n","        loss = nn.BCEWithLogitsLoss()(y_hat, y.float())  # Calculate loss\n","\n","        if opt:    # Perform gradient descent\n","            opt.zero_grad()   # Prevent gradient accumulation\n","            loss.backward()   # Calculate gradient\n","            opt.step()        # Make a step on the weights\n","\n","        total_err += ((y_hat > 0) * (y==0) + (y_hat < 0) * (y==1)).sum().item() # error for 0 and 1 targets\n","        total_loss += loss.item() * X.shape[0]\n","\n","    return total_err / len(loader.dataset), total_loss / len(loader.dataset)"],"metadata":{"id":"bO2aNwvs-SWv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","**Prob 1:**\n","\n","Train a 2-layer linear network with ReLU activations to identify 0s and 1s from MNIST. What did you use as the training set (what percentage)?  How 'wide' was your network?\n","\n"],"metadata":{"id":"gUeKmfivDQPR"}},{"cell_type":"code","source":["################ Prob 1 ################\n","\n","class TwoLayerModel(nn.Module):\n","    def __init__(self):   # Initialize the layers\n","        super(TwoLayerModel, self).__init__()\n","        self.linear1 = nn.Linear(784,512)\n","        self.linear2 = nn.Linear(512,256)\n","        self.linear3 = nn.Linear(256, 1)\n","\n","    def forward(self, x): # Pass input through\n","       x = F.relu(self.linear1(x))\n","       x = F.relu(self.linear2(x))\n","       return self.linear3(x)\n","\n","twoLayerModel = TwoLayerModel()\n","opt = optim.SGD(twoLayerModel.parameters(), lr=0.1)\n","\n","print(\"Train Err\", \"Train Loss\", \"Test Err\", \"Test Loss\", sep=\"\\t\")\n","for i in range(10):\n","    train_err, train_loss = epoch(train_loader, twoLayerModel, opt)\n","    test_err, test_loss = epoch(test_loader, twoLayerModel)\n","    print(*(\"{:.6f}\".format(i) for i in (train_err, train_loss, test_err, test_loss)), sep=\"\\t\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"--3RculF_Xzl","executionInfo":{"status":"ok","timestamp":1726529060423,"user_tz":360,"elapsed":27643,"user":{"displayName":"Danny Perkins","userId":"13093065982105422940"}},"outputId":"2a8766d2-dbc4-4de7-c75c-01da5744f74c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Err\tTrain Loss\tTest Err\tTest Loss\n","0.010896\t0.105598\t0.000946\t0.006438\n","0.001895\t0.008260\t0.000473\t0.003616\n","0.001579\t0.005896\t0.000473\t0.002884\n","0.001184\t0.004803\t0.000473\t0.002574\n","0.001184\t0.004209\t0.000473\t0.002594\n","0.001026\t0.003647\t0.000946\t0.002561\n","0.001026\t0.003399\t0.000473\t0.002367\n","0.000790\t0.003084\t0.000946\t0.002162\n","0.000711\t0.002821\t0.000946\t0.002142\n","0.000632\t0.002677\t0.000946\t0.002112\n"]}]},{"cell_type":"markdown","source":["In the above cell is the code I used to train a two layer network on MNIST. For some context, the MNIST dataset is loaded in before with an 80%-20% training-test split. The network uses the ReLU activation function (only once to avoid its use on the final layer). I chose to use a width of 256 (meaning that the hidden layer had 256 units).\n","\n","As you can see, the training error steadily decreased. But the test error did not get belo 0.000473. So, it was very accurate but not perfect. This is likely due to the simple nature of the dataset. If our data was more complicated, our model would likely not have worked as well.\n","\n","Note: I assume the 2-layers means 2 hidden layers and one final layer. If not, the differences that I could make to the code above to use two hidden layers are pretty trivial anyway (see below)."],"metadata":{"id":"ZpfsjUpBCGDa"}},{"cell_type":"markdown","source":["**Prob 2:**\n","\n","Now try to train a deeper network (4+ layers) to do the same thing, but this time feel free to use different activation functions.  Can you make the size of each layer smaller, i.e. make it less wide?"],"metadata":{"id":"TOQE3HxrDxqC"}},{"cell_type":"code","source":["################ Prob 2 ################\n","\n","class FiveLayerModel(nn.Module):\n","    def __init__(self):   # Initialize the layers\n","        super(FiveLayerModel, self).__init__()\n","        self.linear1 = nn.Linear(784,1024)\n","        self.linear2 = nn.Linear(1024, 512)\n","        self.linear3 = nn.Linear(512, 256)\n","        self.linear4 = nn.Linear(256, 128)\n","        self.linear5 = nn.Linear(128, 1)\n","\n","    def forward(self, x): # Pass input through\n","       x = F.hardtanh(self.linear1(x))\n","       x = F.leaky_relu(self.linear2(x))\n","       x = F.sigmoid(self.linear3(x))\n","       x = F.hardswish(self.linear4(x))\n","       return self.linear5(x)\n","\n","fiveLayerModel = FiveLayerModel()\n","opt = optim.SGD(fiveLayerModel.parameters(), lr=0.1)\n","\n","print(\"Train Err\", \"Train Loss\", \"Test Err\", \"Test Loss\", sep=\"\\t\")\n","for i in range(10):\n","    train_err, train_loss = epoch(train_loader, fiveLayerModel, opt)\n","    test_err, test_loss = epoch(test_loader, fiveLayerModel)\n","    print(*(\"{:.6f}\".format(i) for i in (train_err, train_loss, test_err, test_loss)), sep=\"\\t\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3NChOvUpD5pb","executionInfo":{"status":"ok","timestamp":1726293066340,"user_tz":360,"elapsed":33488,"user":{"displayName":"Danny Perkins","userId":"13093065982105422940"}},"outputId":"c16c42a9-a40a-4aeb-c02a-2b4b7d2d1c27"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Err\tTrain Loss\tTest Err\tTest Loss\n","0.432452\t0.675094\t0.051064\t0.577297\n","0.008843\t0.101627\t0.002364\t0.008344\n","0.002764\t0.009281\t0.000946\t0.004433\n","0.002369\t0.006711\t0.000473\t0.003092\n","0.001816\t0.005374\t0.000473\t0.002454\n","0.001579\t0.004465\t0.000473\t0.002182\n","0.001263\t0.003766\t0.000473\t0.002093\n","0.001421\t0.003387\t0.000473\t0.001873\n","0.000869\t0.002815\t0.000473\t0.001870\n","0.000790\t0.002721\t0.000473\t0.001691\n"]}]},{"cell_type":"markdown","source":["This time I trained a deeper network (with 5 layers) on the same data. For fun, I threw in a bunch of different activation functions (from tanh, to leakyReLu, to sigmoid, to swish). Also, each layer got less and less wide (with the idea that they will learn more global features later on in the network).\n","\n","Since the data is easily separable, we have similar testing errors to the previous model. However, one thing to note is that the training error is less than before, meaning that the model is doing better at classifying images that it trains on."],"metadata":{"id":"MN3IhlO9FVAh"}},{"cell_type":"markdown","source":["**Prob 3:**\n","\n","Now train a 2-layer convolutional network to do the same thing as the previous 2 problems.  Does this train more efficiently?"],"metadata":{"id":"mfMawRsXGPZa"}},{"cell_type":"code","source":["# do a single pass over the data\n","def epoch2(loader, model, opt=None):   # Modify this function to work for 2d input\n","    total_loss, total_err = 0., 0.\n","    for X, y in loader:\n","        y_hat = model(X).squeeze(1)  # Make prediction\n","        loss = nn.BCEWithLogitsLoss()(y_hat, y.float())  # Calculate loss\n","\n","        if opt:    # Perform gradient descent\n","            opt.zero_grad()   # Prevent gradient accumulation\n","            loss.backward()   # Calculate gradient\n","            opt.step()        # Make a step on the weights\n","\n","        total_err += ((y_hat > 0) * (y==0) + (y_hat <= 0) * (y==1)).sum().item() # error for 0 and 1 targets\n","        total_loss += loss.item() * X.shape[0]\n","\n","    return total_err / len(loader.dataset), total_loss / len(loader.dataset)"],"metadata":{"id":"ToWGexZdI9QL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################ Prob 3 ################\n","\n","class TwoLayerConvModel(nn.Module):\n","    def __init__(self):   # Initialize the layers\n","        super(TwoLayerConvModel, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n","        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n","        self.fc1 = nn.Linear(32*7*7, 1)\n","\n","    def forward(self, x): # Pass input through\n","        x = F.relu(self.conv1(x))   # First convolutional layer\n","        x = nn.MaxPool2d(kernel_size=2, stride=2)(x)  # Max pooling layer\n","        x = F.relu(self.conv2(x))   # Second convolutional layer\n","        x = nn.MaxPool2d(kernel_size=2, stride=2)(x)  # Max pooling layer\n","        x = x.view(-1, 32*7*7)     # Flatten the output from the conv layers\n","        return self.fc1(x)    # Fully connected layer\n","\n","twoLayerConvModel = TwoLayerConvModel()\n","opt = optim.SGD(twoLayerConvModel.parameters(), lr=0.1)\n","\n","print(\"Train Err\", \"Train Loss\", \"Test Err\", \"Test Loss\", sep=\"\\t\")\n","for i in range(10):\n","    train_err, train_loss = epoch2(train_loader, twoLayerConvModel, opt)\n","    test_err, test_loss = epoch2(test_loader, twoLayerConvModel)\n","    print(*(\"{:.6f}\".format(i) for i in (train_err, train_loss, test_err, test_loss)), sep=\"\\t\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h9_Y5D0EGWg8","executionInfo":{"status":"ok","timestamp":1726346753229,"user_tz":360,"elapsed":121933,"user":{"displayName":"Danny Perkins","userId":"13093065982105422940"}},"outputId":"f7e2667a-7ce1-49c8-99b5-6892d664ebf7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Err\tTrain Loss\tTest Err\tTest Loss\n","0.014133\t0.052054\t0.001891\t0.004624\n","0.002448\t0.008310\t0.000946\t0.002688\n","0.001895\t0.005422\t0.000946\t0.002227\n","0.001500\t0.004253\t0.000946\t0.002132\n","0.001342\t0.003377\t0.000946\t0.002482\n","0.000711\t0.003031\t0.000946\t0.002053\n","0.000790\t0.002659\t0.000946\t0.002180\n","0.000790\t0.002573\t0.000946\t0.001900\n","0.000790\t0.002440\t0.000946\t0.002045\n","0.000711\t0.002274\t0.000946\t0.002086\n"]}]},{"cell_type":"markdown","source":["This time, I trained a two-layer convolutional neural network (with a fully connected layer at the end to classify). The first layer outputted 16 pannels and the second 32. I chose the kernel size of the convolutions to be 5. Also, I implemented two pooling layers, with a stride and kernel size of 2. To be honest, these choices were somewhat arbitrary.\n","\n","In this training, we got the training error to be lower than it was for the previous networks. But, to my surprise, the test error was actually a bit larger at the end. This means that it likely overfit the data (or at least more than the previous models). I assume that this model would work better if we were to use a larger data set."],"metadata":{"id":"3pm0Q_t_O0A2"}},{"cell_type":"markdown","source":["**Prob 4:**\n","\n","Now consider a 'deep' convolutional network, i.e. one with 4+ layers.  How does this change things?"],"metadata":{"id":"Xtsxoo-7O0mW"}},{"cell_type":"code","source":["################ Prob 3 ################\n","\n","class FourLayerConvModel(nn.Module):\n","    def __init__(self):   # Initialize the layers\n","        super(FourLayerConvModel, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=7, stride=1, padding=2)\n","        self.conv2 = nn.Conv2d(in_channels=4, out_channels=4, kernel_size=5, stride=1, padding=2)\n","        self.conv3 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=5, stride=1, padding=2)\n","        self.conv4 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=5, stride=1, padding=2)\n","        self.conv5 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=2)\n","        self.conv6 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=2)\n","        self.fc1 = nn.Linear(32*30*30, 64)\n","        self.fc2 = nn.Linear(64, 1)\n","\n","    def forward(self, x): # Pass input through\n","        x = F.relu(self.conv1(x))   # First convolutional layer\n","        x = F.relu(self.conv2(x))   # Second convolutional layer\n","        x = F.relu(self.conv3(x))   # Third convolutional layer\n","        x = F.relu(self.conv4(x))   # Fourth convolutional layer\n","        x = F.relu(self.conv5(x))   # Fifth convolutional layer\n","        x = F.relu(self.conv6(x))   # Sixth convolutional layer\n","\n","        x = x.view(-1, 32*30*30)   # Flatten out\n","        x = F.relu(self.fc1(x))     # Fully connected layer 1\n","        return self.fc2(x)    # Fully connected layer 2\n","\n","fourLayerConvModel = FourLayerConvModel()\n","opt = optim.SGD(fourLayerConvModel.parameters(), lr=0.1)\n","\n","print(\"Train Err\", \"Train Loss\", \"Test Err\", \"Test Loss\", sep=\"\\t\")\n","for i in range(10):\n","    train_err, train_loss = epoch2(train_loader, fourLayerConvModel, opt)\n","    test_err, test_loss = epoch2(test_loader, fourLayerConvModel)\n","    print(*(\"{:.6f}\".format(i) for i in (train_err, train_loss, test_err, test_loss)), sep=\"\\t\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hqpIjmSRTN2i","executionInfo":{"status":"ok","timestamp":1726348951455,"user_tz":360,"elapsed":396192,"user":{"displayName":"Danny Perkins","userId":"13093065982105422940"}},"outputId":"c3286e3d-52de-48b4-aaee-61ddde1a52e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Err\tTrain Loss\tTest Err\tTest Loss\n","0.458508\t0.688395\t0.153664\t0.623597\n","0.089143\t0.168663\t0.002364\t0.006737\n","0.003079\t0.011256\t0.001891\t0.004425\n","0.001342\t0.005204\t0.001891\t0.003893\n","0.001263\t0.004473\t0.001891\t0.003913\n","0.001184\t0.003671\t0.001891\t0.002525\n","0.000790\t0.002955\t0.001418\t0.003153\n","0.000790\t0.002442\t0.001891\t0.003338\n","0.000790\t0.002308\t0.001891\t0.002443\n","0.000632\t0.001473\t0.000946\t0.001760\n"]}]},{"cell_type":"markdown","source":["This time I trained a deep convolutional network. It had 6 convolutional layers where the kernels gradually decreased in size and the number of panels gradually increased. Finally, the network finishes with a two-layer neural network. To reduce maintain complexity, I chose to not include any pooling layers.\n","\n","This is a very deep network. So, it took a significant amount of time to train (unlike the previous models). And, for some reason it started with pretty high errors. But, the training error consistently decreased (more so than other models).\n","\n","To my surprise, although the final training error was the lowest for this model, the final test error was actually higher than (or equal to) the other models. I think that this is because the model was overly complicated for the simple task, encouraging it to overfit the data. This model would likely work more effectively if we had a larger dataset."],"metadata":{"id":"E1O6xKX9WuPc"}}]}